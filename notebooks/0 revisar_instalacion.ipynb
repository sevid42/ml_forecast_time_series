{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e912dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373c321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 2.3.3\n",
      "NumPy: 2.3.4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a5a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40b128eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo: C:\\Users\\fierr\\OneDrive\\Documents\\UDEMY\\forecastWallmark\\forecast\\ml_forecast_time_series\\data\\raw\\train.csv\n",
      "‚úì Archivo cargado: 913,000 filas, 4 columnas\n",
      "\n",
      "Analizando columnas...\n",
      "‚úì An√°lisis completado\n",
      "\n",
      "====================================================================================================\n",
      "AN√ÅLISIS DE COLUMNAS\n",
      "====================================================================================================\n",
      "Columna         Tipo  Total Valores  Nulos  Valores Distintos  Valores Blancos  Valores Cero  Valores Negativos                                                   Ejemplos\n",
      "   date Alfanum√©rica         913000      0               1826                0             0                  0 2013-01-01, 2013-01-02, 2013-01-03, 2013-01-04, 2013-01-05\n",
      "  store     Num√©rica         913000      0                 10                0             0                  0                                              1, 2, 3, 4, 5\n",
      "   item     Num√©rica         913000      0                 50                0             0                  0                                              1, 2, 3, 4, 5\n",
      "  sales     Num√©rica         913000      0                213                0             1                  0                                         13, 11, 14, 10, 12\n",
      "Analizando archivo por chunks de 50,000 filas...\n",
      "  Procesados 500,000 filas...\n",
      "‚úì An√°lisis completado: 913,000 filas totales\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columna</th>\n",
       "      <th>Tipo</th>\n",
       "      <th>Total Valores</th>\n",
       "      <th>Nulos</th>\n",
       "      <th>Valores Distintos</th>\n",
       "      <th>Valores Blancos</th>\n",
       "      <th>Valores Cero</th>\n",
       "      <th>Valores Negativos</th>\n",
       "      <th>Ejemplos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date</td>\n",
       "      <td>Alfanum√©rica</td>\n",
       "      <td>913000</td>\n",
       "      <td>0</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01, 2013-01-02, 2013-01-03, 2013-01-04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>store</td>\n",
       "      <td>Num√©rica</td>\n",
       "      <td>913000</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1, 2, 3, 4, 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>item</td>\n",
       "      <td>Num√©rica</td>\n",
       "      <td>913000</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1, 2, 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sales</td>\n",
       "      <td>Num√©rica</td>\n",
       "      <td>913000</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13, 11, 14, 10, 12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Columna          Tipo  Total Valores  Nulos  Valores Distintos  \\\n",
       "0    date  Alfanum√©rica         913000      0               1826   \n",
       "1   store      Num√©rica         913000      0                 10   \n",
       "2    item      Num√©rica         913000      0                 50   \n",
       "3   sales      Num√©rica         913000      0                213   \n",
       "\n",
       "   Valores Blancos  Valores Cero  Valores Negativos  \\\n",
       "0                0             0                  0   \n",
       "1                0             0                  0   \n",
       "2                0             0                  0   \n",
       "3                0             1                  0   \n",
       "\n",
       "                                            Ejemplos  \n",
       "0  2013-01-01, 2013-01-02, 2013-01-03, 2013-01-04...  \n",
       "1                                      1, 2, 3, 4, 5  \n",
       "2                                            1, 2, 3  \n",
       "3                                 13, 11, 14, 10, 12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'reports'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 242\u001b[39m\n\u001b[32m    239\u001b[39m display(df_analisis)\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# Guardar resultado\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[43mdf_analisis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreports/analisis_columnas.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úì An√°lisis guardado en: reports/analisis_columnas.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fierr\\.conda\\envs\\ml_forecast_time_series\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fierr\\.conda\\envs\\ml_forecast_time_series\\Lib\\site-packages\\pandas\\core\\generic.py:3989\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3978\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3980\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3981\u001b[39m     frame=df,\n\u001b[32m   3982\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3986\u001b[39m     decimal=decimal,\n\u001b[32m   3987\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3989\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4006\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fierr\\.conda\\envs\\ml_forecast_time_series\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fierr\\.conda\\envs\\ml_forecast_time_series\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fierr\\.conda\\envs\\ml_forecast_time_series\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fierr\\.conda\\envs\\ml_forecast_time_series\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: 'reports'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analizar_dataframe(df):\n",
    "    \"\"\"\n",
    "    An√°lisis eficiente de DataFrame usando operaciones vectorizadas.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame a analizar\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame con el an√°lisis de cada columna\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pre-calcular informaci√≥n com√∫n para todas las columnas\n",
    "    total_filas = len(df)\n",
    "    \n",
    "    # Lista para almacenar resultados\n",
    "    resultados = []\n",
    "    \n",
    "    for columna in df.columns:\n",
    "        col_data = df[columna]\n",
    "        \n",
    "        # Tipo de dato (m√°s eficiente con select_dtypes)\n",
    "        es_numerica = pd.api.types.is_numeric_dtype(col_data)\n",
    "        tipo = 'Num√©rica' if es_numerica else 'Alfanum√©rica'\n",
    "        \n",
    "        # M√°scara de nulos (calcular una sola vez)\n",
    "        mask_nulos = col_data.isna()\n",
    "        cantidad_nulos = mask_nulos.sum()\n",
    "        \n",
    "        # Valores distintos (incluidos null) - muy eficiente\n",
    "        valores_distintos = col_data.nunique(dropna=False)\n",
    "        \n",
    "        # Inicializar contadores\n",
    "        cantidad_blancos = 0\n",
    "        cantidad_ceros = 0\n",
    "        cantidad_negativos = 0\n",
    "        \n",
    "        if es_numerica:\n",
    "            # Operaciones vectorizadas para num√©ricos (muy r√°pido)\n",
    "            col_no_nulos = col_data.dropna()\n",
    "            if len(col_no_nulos) > 0:\n",
    "                cantidad_ceros = (col_no_nulos == 0).sum()\n",
    "                cantidad_negativos = (col_no_nulos < 0).sum()\n",
    "        else:\n",
    "            # Para alfanum√©ricos, usar operaciones de string vectorizadas\n",
    "            # M√°s eficiente que apply\n",
    "            if col_data.dtype == 'object':\n",
    "                # Filtrar solo strings y contar blancos en una operaci√≥n\n",
    "                mask_strings = col_data.apply(lambda x: isinstance(x, str))\n",
    "                if mask_strings.any():\n",
    "                    cantidad_blancos = col_data[mask_strings].str.strip().eq('').sum()\n",
    "        \n",
    "        # 5 valores de ejemplo (optimizado)\n",
    "        valores_unicos = col_data.dropna().unique()\n",
    "        valores_ejemplo = valores_unicos[:5] if len(valores_unicos) > 0 else []\n",
    "        valores_ejemplo_str = ', '.join(map(str, valores_ejemplo))\n",
    "        \n",
    "        # Agregar resultados\n",
    "        resultados.append({\n",
    "            'Columna': columna,\n",
    "            'Tipo': tipo,\n",
    "            'Total Valores': total_filas,\n",
    "            'Nulos': int(cantidad_nulos),\n",
    "            'Valores Distintos': int(valores_distintos),\n",
    "            'Valores Blancos': int(cantidad_blancos),\n",
    "            'Valores Cero': int(cantidad_ceros),\n",
    "            'Valores Negativos': int(cantidad_negativos),\n",
    "            'Ejemplos': valores_ejemplo_str\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "\n",
    "def analizar_csv(ruta_archivo, separador=',', encoding='utf-8', nrows=None):\n",
    "    \"\"\"\n",
    "    Lee y analiza un archivo CSV de forma eficiente.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ruta_archivo : str\n",
    "        Ruta del archivo CSV\n",
    "    separador : str, default=','\n",
    "        Separador del CSV\n",
    "    encoding : str, default='utf-8'\n",
    "        Codificaci√≥n del archivo\n",
    "    nrows : int, optional\n",
    "        N√∫mero de filas a leer (√∫til para archivos grandes)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (DataFrame original, DataFrame de an√°lisis)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Leyendo archivo: {ruta_archivo}\")\n",
    "    \n",
    "    # Leer CSV con optimizaciones\n",
    "    df = pd.read_csv(\n",
    "        ruta_archivo, \n",
    "        sep=separador, \n",
    "        encoding=encoding,\n",
    "        nrows=nrows,\n",
    "        low_memory=False  # M√°s r√°pido para archivos grandes\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Archivo cargado: {df.shape[0]:,} filas, {df.shape[1]} columnas\\n\")\n",
    "    \n",
    "    print(\"Analizando columnas...\")\n",
    "    df_analisis = analizar_dataframe(df)\n",
    "    print(\"‚úì An√°lisis completado\\n\")\n",
    "    \n",
    "    return df, df_analisis\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# VERSI√ìN ULTRA OPTIMIZADA (para Big Data)\n",
    "# ============================================\n",
    "\n",
    "def analizar_csv_chunked(ruta_archivo, chunksize=10000, separador=',', encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Analiza archivos CSV muy grandes por chunks (evita cargar todo en memoria).\n",
    "    Ideal para datasets de millones de filas.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ruta_archivo : str\n",
    "        Ruta del archivo CSV\n",
    "    chunksize : int, default=10000\n",
    "        N√∫mero de filas por chunk\n",
    "    separador : str, default=','\n",
    "        Separador del CSV\n",
    "    encoding : str, default='utf-8'\n",
    "        Codificaci√≥n del archivo\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame con el an√°lisis de cada columna\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Analizando archivo por chunks de {chunksize:,} filas...\")\n",
    "    \n",
    "    # Diccionarios para acumular estad√≠sticas\n",
    "    stats = {}\n",
    "    total_rows = 0\n",
    "    \n",
    "    # Leer por chunks\n",
    "    for i, chunk in enumerate(pd.read_csv(ruta_archivo, sep=separador, \n",
    "                                          encoding=encoding, chunksize=chunksize)):\n",
    "        \n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "        for columna in chunk.columns:\n",
    "            if columna not in stats:\n",
    "                # Inicializar estad√≠sticas para nueva columna\n",
    "                es_numerica = pd.api.types.is_numeric_dtype(chunk[columna])\n",
    "                stats[columna] = {\n",
    "                    'tipo': 'Num√©rica' if es_numerica else 'Alfanum√©rica',\n",
    "                    'nulos': 0,\n",
    "                    'valores_unicos': set(),\n",
    "                    'blancos': 0,\n",
    "                    'ceros': 0,\n",
    "                    'negativos': 0,\n",
    "                    'ejemplos': []\n",
    "                }\n",
    "            \n",
    "            col_data = chunk[columna]\n",
    "            col_stats = stats[columna]\n",
    "            \n",
    "            # Acumular nulos\n",
    "            col_stats['nulos'] += col_data.isna().sum()\n",
    "            \n",
    "            # Acumular valores √∫nicos (limitado para eficiencia)\n",
    "            if len(col_stats['valores_unicos']) < 10000:\n",
    "                col_stats['valores_unicos'].update(col_data.dropna().unique())\n",
    "            \n",
    "            # Ejemplos (solo del primer chunk)\n",
    "            if i == 0 and len(col_stats['ejemplos']) < 5:\n",
    "                ejemplos = col_data.dropna().unique()[:5].tolist()\n",
    "                col_stats['ejemplos'].extend(ejemplos)\n",
    "            \n",
    "            # Estad√≠sticas espec√≠ficas por tipo\n",
    "            if col_stats['tipo'] == 'Num√©rica':\n",
    "                col_no_nulos = col_data.dropna()\n",
    "                col_stats['ceros'] += (col_no_nulos == 0).sum()\n",
    "                col_stats['negativos'] += (col_no_nulos < 0).sum()\n",
    "            else:\n",
    "                if col_data.dtype == 'object':\n",
    "                    mask_strings = col_data.apply(lambda x: isinstance(x, str))\n",
    "                    if mask_strings.any():\n",
    "                        col_stats['blancos'] += col_data[mask_strings].str.strip().eq('').sum()\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Procesados {total_rows:,} filas...\")\n",
    "    \n",
    "    print(f\"‚úì An√°lisis completado: {total_rows:,} filas totales\\n\")\n",
    "    \n",
    "    # Construir DataFrame de resultados\n",
    "    resultados = []\n",
    "    for columna, col_stats in stats.items():\n",
    "        resultados.append({\n",
    "            'Columna': columna,\n",
    "            'Tipo': col_stats['tipo'],\n",
    "            'Total Valores': total_rows,\n",
    "            'Nulos': int(col_stats['nulos']),\n",
    "            'Valores Distintos': len(col_stats['valores_unicos']),\n",
    "            'Valores Blancos': int(col_stats['blancos']),\n",
    "            'Valores Cero': int(col_stats['ceros']),\n",
    "            'Valores Negativos': int(col_stats['negativos']),\n",
    "            'Ejemplos': ', '.join(map(str, col_stats['ejemplos'][:5]))\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# EJEMPLO DE USO\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ===== PARA ARCHIVOS NORMALES (< 1 GB) =====\n",
    "    ruta_csv = 'C:\\\\Users\\\\fierr\\\\OneDrive\\\\Documents\\\\UDEMY\\\\forecastWallmark\\\\forecast\\\\ml_forecast_time_series\\\\data\\\\raw\\\\train.csv'\n",
    "    df_original, df_analisis = analizar_csv(ruta_csv)\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(\"AN√ÅLISIS DE COLUMNAS\")\n",
    "    print(\"=\" * 100)\n",
    "    print(df_analisis.to_string(index=False))\n",
    "    \n",
    "    # ===== PARA ARCHIVOS GRANDES (> 1 GB) =====\n",
    "    df_analisis = analizar_csv_chunked(ruta_csv, chunksize=50000)\n",
    "    display(df_analisis)\n",
    "    \n",
    "    # Guardar resultado\n",
    "    df_analisis.to_csv('reports/analisis_columnas.csv', index=False)\n",
    "    print(\"\\n‚úì An√°lisis guardado en: reports/analisis_columnas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6225f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# ============================================\n",
    "# AN√ÅLISIS SIMPLE DE SERIES DE TIEMPO\n",
    "# ============================================\n",
    "\n",
    "def analizar_series(df, col_fecha='date', col_store='store', col_item='item'):\n",
    "    \"\"\"\n",
    "    An√°lisis r√°pido de series de tiempo por store e item.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"AN√ÅLISIS DE SERIES DE TIEMPO\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Convertir fecha si es necesario\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[col_fecha]):\n",
    "        df[col_fecha] = pd.to_datetime(df[col_fecha])\n",
    "    \n",
    "    print(f\"üìä Total registros: {len(df):,}\")\n",
    "    print(f\"üè™ Stores √∫nicos: {df[col_store].nunique():,}\")\n",
    "    print(f\"üì¶ Items √∫nicos: {df[col_item].nunique():,}\")\n",
    "    \n",
    "    # An√°lisis por (store, item)\n",
    "    analisis = df.groupby([col_store, col_item]).agg(\n",
    "        registros=pd.NamedAgg(column=col_fecha, aggfunc='count'),\n",
    "        fecha_min=pd.NamedAgg(column=col_fecha, aggfunc='min'),\n",
    "        fecha_max=pd.NamedAgg(column=col_fecha, aggfunc='max')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calcular d√≠as\n",
    "    analisis['dias_totales'] = (analisis['fecha_max'] - analisis['fecha_min']).dt.days + 1\n",
    "    analisis['dias_esperados'] = analisis['dias_totales']\n",
    "    analisis['completitud_pct'] = (analisis['registros'] / analisis['dias_esperados'] * 100).round(2)\n",
    "    analisis['registros_faltantes'] = analisis['dias_esperados'] - analisis['registros']\n",
    "    \n",
    "    print(f\"üî¢ Combinaciones (store, item): {len(analisis):,}\\n\")\n",
    "    \n",
    "    return analisis\n",
    "\n",
    "\n",
    "def analizar_por_deciles(analisis_df):\n",
    "    \"\"\"\n",
    "    Agrupa las series por deciles seg√∫n el n√∫mero de registros.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcular deciles\n",
    "    try:\n",
    "        analisis_df['decil'] = pd.qcut(\n",
    "            analisis_df['registros'], \n",
    "            q=10, \n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        # Convertir a etiquetas D1, D2, etc.\n",
    "        analisis_df['decil'] = 'D' + (analisis_df['decil'] + 1).astype(str)\n",
    "    except Exception:\n",
    "        # Si no se pueden calcular 10 deciles, usar percentiles\n",
    "        analisis_df['decil'] = pd.cut(\n",
    "            analisis_df['registros'],\n",
    "            bins=10,\n",
    "            labels=['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10']\n",
    "        )\n",
    "    \n",
    "    # Estad√≠sticas por decil\n",
    "    deciles = analisis_df.groupby('decil', observed=True).agg({\n",
    "        'store': 'count',\n",
    "        'registros': ['min', 'max', 'mean'],\n",
    "        'completitud_pct': 'mean',\n",
    "        'registros_faltantes': ['mean', 'sum']\n",
    "    }).round(2)\n",
    "    \n",
    "    deciles.columns = ['num_series', 'registros_min', 'registros_max', \n",
    "                      'registros_prom', 'completitud_prom', \n",
    "                      'faltantes_prom', 'faltantes_total']\n",
    "    \n",
    "    return deciles, analisis_df\n",
    "\n",
    "\n",
    "def mostrar_resultados(analisis_df, deciles_df):\n",
    "    \"\"\"\n",
    "    Muestra los resultados del an√°lisis.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"RESUMEN ESTAD√çSTICO\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    resumen_data = {\n",
    "        'M√©trica': [\n",
    "            'Combinaciones (store, item)',\n",
    "            'Registros promedio',\n",
    "            'Registros m√≠nimo',\n",
    "            'Registros m√°ximo',\n",
    "            'Completitud promedio',\n",
    "            'Series 100% completas',\n",
    "            'Series con gaps'\n",
    "        ],\n",
    "        'Valor': [\n",
    "            f\"{len(analisis_df):,}\",\n",
    "            f\"{analisis_df['registros'].mean():.0f}\",\n",
    "            f\"{analisis_df['registros'].min():,}\",\n",
    "            f\"{analisis_df['registros'].max():,}\",\n",
    "            f\"{analisis_df['completitud_pct'].mean():.1f}%\",\n",
    "            f\"{(analisis_df['completitud_pct'] == 100).sum():,} ({(analisis_df['completitud_pct'] == 100).sum()/len(analisis_df)*100:.1f}%)\",\n",
    "            f\"{(analisis_df['registros_faltantes'] > 0).sum():,} ({(analisis_df['registros_faltantes'] > 0).sum()/len(analisis_df)*100:.1f}%)\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(tabulate(pd.DataFrame(resumen_data), headers='keys', tablefmt='grid', showindex=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AN√ÅLISIS POR DECILES (seg√∫n n√∫mero de registros)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    print(tabulate(deciles_df, headers='keys', tablefmt='grid'))\n",
    "    \n",
    "    # Distribuci√≥n de completitud\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DISTRIBUCI√ìN DE COMPLETITUD\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    bins_completitud = [0, 50, 70, 80, 90, 95, 99, 100]\n",
    "    analisis_df['rango_completitud'] = pd.cut(\n",
    "        analisis_df['completitud_pct'], \n",
    "        bins=bins_completitud,\n",
    "        labels=['0-50%', '50-70%', '70-80%', '80-90%', '90-95%', '95-99%', '99-100%']\n",
    "    )\n",
    "    \n",
    "    dist_completitud = analisis_df['rango_completitud'].value_counts().sort_index()\n",
    "    dist_completitud_pct = (dist_completitud / len(analisis_df) * 100).round(2)\n",
    "    \n",
    "    completitud_tabla = pd.DataFrame({\n",
    "        'Rango Completitud': dist_completitud.index,\n",
    "        'Num Series': dist_completitud.values,\n",
    "        '% del Total': dist_completitud_pct.values\n",
    "    })\n",
    "    \n",
    "    print(tabulate(completitud_tabla, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FUNCI√ìN PRINCIPAL - USO R√ÅPIDO\n",
    "# ============================================\n",
    "\n",
    "def analisis_completo(df, exportar=True):\n",
    "    \"\"\"\n",
    "    Ejecuta an√°lisis completo y exporta resultados.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame con columnas: date, store, item\n",
    "    exportar : bool\n",
    "        Si True, exporta a CSV\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (analisis_df, deciles_df)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. An√°lisis por serie\n",
    "    analisis_df = analizar_series(df)\n",
    "    \n",
    "    # 2. An√°lisis por deciles\n",
    "    deciles_df, analisis_df = analizar_por_deciles(analisis_df)\n",
    "    \n",
    "    # 3. Mostrar resultados\n",
    "    mostrar_resultados(analisis_df, deciles_df)\n",
    "    \n",
    "    # 4. Exportar\n",
    "    if exportar:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXPORTANDO RESULTADOS\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        analisis_df.to_csv('analisis_series_completo.csv', index=False)\n",
    "        print(\"‚úì analisis_series_completo.csv\")\n",
    "        \n",
    "        deciles_df.to_csv('analisis_deciles.csv')\n",
    "        print(\"‚úì analisis_deciles.csv\")\n",
    "        \n",
    "        # Exportar series con problemas\n",
    "        series_problemas = analisis_df[analisis_df['completitud_pct'] < 90]\n",
    "        if len(series_problemas) > 0:\n",
    "            series_problemas.to_csv('series_incompletas.csv', index=False)\n",
    "            print(\"‚úì series_incompletas.csv\")\n",
    "    \n",
    "    return analisis_df, deciles_df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# EJEMPLO DE USO CON TU DATASET\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Ruta de tu archivo\n",
    "    ruta_archivo = r'C:\\Users\\fierr\\OneDrive\\Documents\\UDEMY\\forecastWallmark\\forecast\\ml_forecast_time_series\\data\\raw\\train.csv'\n",
    "    \n",
    "    # Schema de tipos\n",
    "    raw_schema = {\n",
    "        'store': 'Int64',\n",
    "        'item': 'Int64',\n",
    "        'sales': 'Int64'\n",
    "    }\n",
    "    \n",
    "    # Schema de fechas\n",
    "    date_schema = {\n",
    "        'date': '%Y-%m-%d'\n",
    "    }\n",
    "    \n",
    "    # Leer CSV\n",
    "    print(\"üìÇ Leyendo CSV...\\n\")\n",
    "    \n",
    "    # Opci√≥n 1: Lectura simple\n",
    "    df = pd.read_csv(ruta_archivo, dtype=raw_schema, parse_dates=['date'])\n",
    "    \n",
    "    # O usando tu funci√≥n personalizada:\n",
    "    # from tu_funcion_mejorada import leer_csv\n",
    "    # df = leer_csv(ruta_archivo, df_schema=raw_schema, date_schema=date_schema)\n",
    "    \n",
    "    print(f\"‚úì Datos cargados: {df.shape[0]:,} registros\\n\")\n",
    "    \n",
    "    # Ejecutar an√°lisis completo\n",
    "    analisis_df, deciles_df = analisis_completo(df, exportar=True)\n",
    "    \n",
    "    # ========================================\n",
    "    # CONSULTAS ADICIONALES\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CONSULTAS ADICIONALES\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Top 10 series con m√°s registros\n",
    "    print(\"üìä TOP 10 SERIES CON M√ÅS REGISTROS:\")\n",
    "    top_10 = analisis_df.nlargest(10, 'registros')[['store', 'item', 'registros', 'completitud_pct']]\n",
    "    print(tabulate(top_10, headers='keys', tablefmt='grid', showindex=False))\n",
    "    \n",
    "    print(\"\\nüìä TOP 10 SERIES CON MENOS REGISTROS:\")\n",
    "    bottom_10 = analisis_df.nsmallest(10, 'registros')[['store', 'item', 'registros', 'completitud_pct']]\n",
    "    print(tabulate(bottom_10, headers='keys', tablefmt='grid', showindex=False))\n",
    "    \n",
    "    # Series por decil\n",
    "    print(\"\\nüìä N√öMERO DE SERIES POR DECIL:\")\n",
    "    series_por_decil = analisis_df['decil'].value_counts().sort_index()\n",
    "    print(tabulate(series_por_decil.reset_index(), headers=['Decil', 'Num Series'], \n",
    "                  tablefmt='grid', showindex=False))\n",
    "    \n",
    "    # Ejemplo: filtrar serie espec√≠fica\n",
    "    print(\"\\nüìä EJEMPLO: Serie Store=1, Item=1:\")\n",
    "    serie_ejemplo = analisis_df[(analisis_df['store'] == 1) & (analisis_df['item'] == 1)]\n",
    "    if len(serie_ejemplo) > 0:\n",
    "        print(tabulate(serie_ejemplo[['store', 'item', 'registros', 'fecha_min', \n",
    "                                     'fecha_max', 'dias_totales', 'completitud_pct']], \n",
    "                      headers='keys', tablefmt='grid', showindex=False))\n",
    "    \n",
    "    # Series por rango de completitud\n",
    "    print(\"\\nüìä SERIES POR RANGO DE COMPLETITUD:\")\n",
    "    completitud_rangos = analisis_df.groupby('rango_completitud', observed=True).size()\n",
    "    print(tabulate(completitud_rangos.reset_index(), headers=['Rango', 'Cantidad'], \n",
    "                  tablefmt='grid', showindex=False))\n",
    "    \n",
    "    # Estad√≠sticas por decil\n",
    "    print(\"\\nüìä ESTAD√çSTICAS POR DECIL:\")\n",
    "    for decil in ['D1', 'D5', 'D10']:\n",
    "        if decil in analisis_df['decil'].values:\n",
    "            series_decil = analisis_df[analisis_df['decil'] == decil]\n",
    "            print(f\"\\n{decil}:\")\n",
    "            print(f\"  - Series: {len(series_decil)}\")\n",
    "            print(f\"  - Registros promedio: {series_decil['registros'].mean():.0f}\")\n",
    "            print(f\"  - Registros min-max: {series_decil['registros'].min()}-{series_decil['registros'].max()}\")\n",
    "            print(f\"  - Completitud promedio: {series_decil['completitud_pct'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f812fd84",
   "metadata": {},
   "source": [
    "# COD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00ad40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_csv(ruta, separador=',', encoding='utf-8', df_schema={}, date_schema={}):\n",
    "    df = pd.read_csv(\n",
    "        ruta,\n",
    "        sep = separador,                          # Separador\n",
    "        encoding = encoding,                 # Codificaci√≥n\n",
    "        dtype = df_schema,                 # ‚Üê AQU√ç VA EL DICCIONARIO\n",
    "        #parse_dates = date,                 # Columnas de fecha\n",
    "        #na_values=valores_nulos,          # Qu√© considerar como nulo\n",
    "        low_memory = False                  # M√°s r√°pido para archivos grandes\n",
    "    )\n",
    "\n",
    "    #convertimos lo que sea fecha\n",
    "    if date_schema:\n",
    "        for col_fecha, formato in date_schema.items():\n",
    "            df[col_fecha] = pd.to_datetime(df[col_fecha], format=formato, errors='coerce')\n",
    "            #print(f\"   ‚úì {col_fecha} ‚Üí datetime ({formato})\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def analizar_csv(df):\n",
    "    # Pre-calcular informaci√≥n com√∫n para todas las columnas\n",
    "    total_filas = len(df)\n",
    "    \n",
    "    # Lista para almacenar resultados\n",
    "    resultados = []\n",
    "    \n",
    "    for columna in df.columns:\n",
    "        col_data = df[columna]\n",
    "        \n",
    "        # Tipo de dato (m√°s eficiente con select_dtypes)\n",
    "        es_numerica = pd.api.types.is_numeric_dtype(col_data)\n",
    "        tipo = 'Num√©rica' if es_numerica else 'Alfanum√©rica'\n",
    "        \n",
    "        # M√°scara de nulos (calcular una sola vez)\n",
    "        mask_nulos = col_data.isna()\n",
    "        cantidad_nulos = mask_nulos.sum()\n",
    "        \n",
    "        # Valores distintos (incluidos null) - muy eficiente\n",
    "        valores_distintos = col_data.nunique(dropna=False)\n",
    "        \n",
    "        # Inicializar contadores\n",
    "        cantidad_blancos = 0\n",
    "        cantidad_ceros = 0\n",
    "        cantidad_negativos = 0\n",
    "        \n",
    "        if es_numerica:\n",
    "            # Operaciones vectorizadas para num√©ricos (muy r√°pido)\n",
    "            col_no_nulos = col_data.dropna()\n",
    "            if len(col_no_nulos) > 0:\n",
    "                cantidad_ceros = (col_no_nulos == 0).sum()\n",
    "                cantidad_negativos = (col_no_nulos < 0).sum()\n",
    "        else:\n",
    "            # Para alfanum√©ricos, usar operaciones de string vectorizadas\n",
    "            # M√°s eficiente que apply\n",
    "            if col_data.dtype == 'object':\n",
    "                # Filtrar solo strings y contar blancos en una operaci√≥n\n",
    "                mask_strings = col_data.apply(lambda x: isinstance(x, str))\n",
    "                if mask_strings.any():\n",
    "                    cantidad_blancos = col_data[mask_strings].str.strip().eq('').sum()\n",
    "        \n",
    "        # 5 valores de ejemplo (optimizado)\n",
    "        valores_unicos = col_data.dropna().unique()\n",
    "        valores_ejemplo = valores_unicos[:5] if len(valores_unicos) > 0 else []\n",
    "        valores_ejemplo_str = ', '.join(map(str, valores_ejemplo))\n",
    "        \n",
    "        # Agregar resultados\n",
    "        resultados.append({\n",
    "            'Columna': columna,\n",
    "            'Tipo': tipo,\n",
    "            'Total Valores': total_filas,\n",
    "            'Nulos': int(cantidad_nulos),\n",
    "            'Valores Distintos': int(valores_distintos),\n",
    "            'Valores Blancos': int(cantidad_blancos),\n",
    "            'Valores Cero': int(cantidad_ceros),\n",
    "            'Valores Negativos': int(cantidad_negativos),\n",
    "            'Ejemplos': valores_ejemplo_str\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96822fe2",
   "metadata": {},
   "source": [
    "## Identificadores:\n",
    "\n",
    "- store: C√≥digo de la tienda\n",
    "- item: C√≥digo del producto\n",
    "\n",
    "Fechas de actividad:\n",
    "\n",
    "- local_start_date: Primera fecha en que este producto se vendi√≥ en esta tienda\n",
    "- local_end_date: √öltima fecha en que este producto se vendi√≥ en esta tienda\n",
    "- general_start_date: Fecha m√°s antigua con ventas en todo el sistema\n",
    "- general_end_date: Fecha m√°s reciente con ventas en todo el sistema\n",
    "\n",
    "Cobertura de datos (per√≠odo propio):\n",
    "\n",
    "- local_periods_total: Total de d√≠as entre la primera y √∫ltima venta del producto en la tienda\n",
    "- local_periods_with_data: D√≠as en que hubo al menos una transacci√≥n registrada\n",
    "- local_periods_missing: D√≠as sin ning√∫n registro entre primera y √∫ltima venta (gaps en los datos)\n",
    "- local_null_sales_count: D√≠as donde hubo registros pero todas las ventas aparecen sin valor (datos incompletos)\n",
    "\n",
    "Cobertura de datos (per√≠odo extendido):\n",
    "\n",
    "- extended_periods_total: D√≠as desde la primera venta del producto hasta la fecha m√°s reciente del sistema\n",
    "- extended_periods_with_data: D√≠as con transacciones en ese per√≠odo extendido\n",
    "- extended_periods_missing: D√≠as sin datos desde que el producto comenz√≥ hasta hoy\n",
    "- extended_null_sales_count: D√≠as con datos incompletos en ese per√≠odo extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04beaec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columna</th>\n",
       "      <th>Tipo</th>\n",
       "      <th>Total Valores</th>\n",
       "      <th>Nulos</th>\n",
       "      <th>Valores Distintos</th>\n",
       "      <th>Valores Blancos</th>\n",
       "      <th>Valores Cero</th>\n",
       "      <th>Valores Negativos</th>\n",
       "      <th>Ejemplos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date</td>\n",
       "      <td>Alfanum√©rica</td>\n",
       "      <td>913000</td>\n",
       "      <td>0</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01 00:00:00, 2013-01-02 00:00:00, 2013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>store</td>\n",
       "      <td>Num√©rica</td>\n",
       "      <td>913000</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1, 2, 3, 4, 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>item</td>\n",
       "      <td>Num√©rica</td>\n",
       "      <td>913000</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1, 2, 3, 4, 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sales</td>\n",
       "      <td>Num√©rica</td>\n",
       "      <td>913000</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13, 11, 14, 10, 12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Columna          Tipo  Total Valores  Nulos  Valores Distintos  \\\n",
       "0    date  Alfanum√©rica         913000      0               1826   \n",
       "1   store      Num√©rica         913000      0                 10   \n",
       "2    item      Num√©rica         913000      0                 50   \n",
       "3   sales      Num√©rica         913000      0                213   \n",
       "\n",
       "   Valores Blancos  Valores Cero  Valores Negativos  \\\n",
       "0                0             0                  0   \n",
       "1                0             0                  0   \n",
       "2                0             0                  0   \n",
       "3                0             1                  0   \n",
       "\n",
       "                                            Ejemplos  \n",
       "0  2013-01-01 00:00:00, 2013-01-02 00:00:00, 2013...  \n",
       "1                                      1, 2, 3, 4, 5  \n",
       "2                                      1, 2, 3, 4, 5  \n",
       "3                                 13, 11, 14, 10, 12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1\n",
      "2013-01-01 00:00:00 - 2017-12-31 00:00:00\n",
      "F2\n",
      "F2.1\n",
      "F2.2\n",
      "F2.3\n",
      "F3\n",
      "F4\n",
      "F5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>local_start_date</th>\n",
       "      <th>local_end_date</th>\n",
       "      <th>local_periods_with_data</th>\n",
       "      <th>local_null_sales_count</th>\n",
       "      <th>general_start_date</th>\n",
       "      <th>general_end_date</th>\n",
       "      <th>local_periods_total</th>\n",
       "      <th>local_periods_missing</th>\n",
       "      <th>extended_periods_total</th>\n",
       "      <th>extended_periods_with_data</th>\n",
       "      <th>extended_periods_missing</th>\n",
       "      <th>extended_null_sales_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>1826</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>1826</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>1826</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>1826</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>1826</td>\n",
       "      <td>1826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store  item local_start_date local_end_date  local_periods_with_data  \\\n",
       "0      1     1       2013-01-01     2017-12-31                     1826   \n",
       "1      1     2       2013-01-01     2017-12-31                     1826   \n",
       "2      1     3       2013-01-01     2017-12-31                     1826   \n",
       "3      1     4       2013-01-01     2017-12-31                     1826   \n",
       "4      1     5       2013-01-01     2017-12-31                     1826   \n",
       "\n",
       "   local_null_sales_count general_start_date general_end_date  \\\n",
       "0                       0         2013-01-01       2017-12-31   \n",
       "1                       0         2013-01-01       2017-12-31   \n",
       "2                       0         2013-01-01       2017-12-31   \n",
       "3                       0         2013-01-01       2017-12-31   \n",
       "4                       0         2013-01-01       2017-12-31   \n",
       "\n",
       "   local_periods_total  local_periods_missing  extended_periods_total  \\\n",
       "0                 1826                      0                    1826   \n",
       "1                 1826                      0                    1826   \n",
       "2                 1826                      0                    1826   \n",
       "3                 1826                      0                    1826   \n",
       "4                 1826                      0                    1826   \n",
       "\n",
       "   extended_periods_with_data  extended_periods_missing  \\\n",
       "0                        1826                         0   \n",
       "1                        1826                         0   \n",
       "2                        1826                         0   \n",
       "3                        1826                         0   \n",
       "4                        1826                         0   \n",
       "\n",
       "   extended_null_sales_count  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analizar_series_tiempo(df, granularidad, col_fecha, cols_id, col_valor):\n",
    "    \"\"\" granularidad:\n",
    "    'D' - D√≠a (daily)\n",
    "    'W' - Semana (weekly)\n",
    "    'M' - Mes (month end)\n",
    "    'MS' - Mes (month start)\n",
    "    'Q' - Trimestre (quarter end)\n",
    "    'QS' - Trimestre (quarter start)\n",
    "    'Y' o 'A' - A√±o (year end)\n",
    "    'YS' o 'AS' - A√±o (year start)\n",
    "\n",
    "    Otros √∫tiles:\n",
    "\n",
    "    'H' - Hora (hourly)\n",
    "    'T' o 'min' - Minuto (minute)\n",
    "    'S' - Segundo (second)\n",
    "    'B' - D√≠a h√°bil (business day)\n",
    "    'W-MON', 'W-TUE', etc. - Semana comenzando en d√≠a espec√≠fico\n",
    "    \"\"\"\n",
    "    print(\"F1\")\n",
    "    # Convertir fecha a datetime y filtrar nulls en fecha\n",
    "    df[col_fecha] = pd.to_datetime(df[col_fecha])\n",
    "    df_clean = df[df[col_fecha].notna()].copy()\n",
    "    \n",
    "    # Fechas globales\n",
    "    general_start_date = df_clean[col_fecha].min()\n",
    "    general_end_date = df_clean[col_fecha].max()\n",
    "    print(general_start_date, '-', general_end_date)\n",
    "    \n",
    "    # Agrupar por cols_id\n",
    "    resultado = df_clean.groupby(cols_id).agg({\n",
    "        col_fecha: ['min', 'max', 'nunique']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Renombrar columnas\n",
    "    # local_periods_with_data considera aun peridos con nulls en sales\n",
    "    resultado.columns = cols_id + ['local_start_date', 'local_end_date', 'local_periods_with_data']\n",
    "    \n",
    "    \n",
    "    print(\"F2\")\n",
    "    # Contar per√≠odos donde TODOS los sales son null (optimizado)\n",
    "    df_temp = df_clean.copy()\n",
    "    df_temp['is_null'] = df_temp[col_valor].isna()\n",
    "    \n",
    "    periods_all_null = df_temp.groupby(cols_id + [col_fecha])['is_null'].all().reset_index(name='all_null')\n",
    "    \n",
    "    print(\"F2.1\")\n",
    "    periods_all_null = periods_all_null.groupby(cols_id)['all_null'].sum().reset_index()\n",
    "    periods_all_null.columns = cols_id + ['local_null_sales_count']\n",
    "    \n",
    "    print(\"F2.2\")\n",
    "    resultado = resultado.merge(periods_all_null, on=cols_id, how='left')\n",
    "    # local_null_sales_count tendr√° la cantidad de periodos en que todos los sales son null\n",
    "    resultado['local_null_sales_count'] = resultado['local_null_sales_count'].fillna(0).astype(int)\n",
    "    \n",
    "    print(\"F2.3\")\n",
    "    # Agregar fechas globales\n",
    "    resultado['general_start_date'] = general_start_date\n",
    "    resultado['general_end_date'] = general_end_date\n",
    "    \n",
    "    print(\"F3\")\n",
    "    # Per√≠odos totales y faltantes (rango local)\n",
    "    resultado['local_periods_total'] = resultado.apply(\n",
    "        lambda row: len(pd.date_range(row['local_start_date'], row['local_end_date'], freq=granularidad.upper())),\n",
    "        axis=1\n",
    "    )\n",
    "    resultado['local_periods_missing'] = resultado['local_periods_total'] - resultado['local_periods_with_data']\n",
    "    \n",
    "    # Per√≠odos totales y faltantes (rango extendido: local_start a general_end)\n",
    "    resultado['extended_periods_total'] = resultado.apply(\n",
    "        lambda row: len(pd.date_range(row['local_start_date'], general_end_date, freq=granularidad.upper())),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    print(\"F4\")\n",
    "    # Los per√≠odos con datos son los mismos porque cada grupo solo tiene datos hasta su local_end\n",
    "    resultado['extended_periods_with_data'] = resultado['local_periods_with_data']\n",
    "    resultado['extended_periods_missing'] = resultado['extended_periods_total'] - resultado['extended_periods_with_data']\n",
    "    \n",
    "    # Los per√≠odos con todos-nulls tambi√©n son los mismos (solo existen hasta local_end)\n",
    "    resultado['extended_null_sales_count'] = resultado['local_null_sales_count']\n",
    "    \n",
    "    print(\"F5\")\n",
    "    display(resultado.head())\n",
    "    return resultado\n",
    "\n",
    "\n",
    "    \n",
    "    #cantidad de \n",
    "\n",
    "    #cuantas series \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ruta_archivo = 'C:\\\\Users\\\\fierr\\\\OneDrive\\\\Documents\\\\UDEMY\\\\forecastWallmark\\\\forecast\\\\ml_forecast_time_series\\\\data\\\\raw\\\\train.csv'\n",
    "    separador=','\n",
    "    encoding='utf-8'\n",
    "\n",
    "    raw_schema={\n",
    "        'date': 'string',\n",
    "        'store': 'Int64',        # Con may√∫scula acepta nulos\n",
    "        'item': 'Int64',\n",
    "        'sales': 'Int64'\n",
    "    }\n",
    "\n",
    "    date_schema={'date': '%Y-%m-%d'}\n",
    "\n",
    "    df = leer_csv(ruta_archivo,separador,encoding,raw_schema,date_schema)\n",
    "    df_analisis= analizar_csv(df.copy())\n",
    "    display(df_analisis)\n",
    "    \n",
    "    df_analisis_series = analizar_series_tiempo(df.copy(), 'd','date', ['store','item'], 'sales')\n",
    "    #Validar ingresando unas ingreso de un registro con 4 valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af36f3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>local_start_date</th>\n",
       "      <th>local_end_date</th>\n",
       "      <th>local_periods_with_data</th>\n",
       "      <th>local_null_sales_count</th>\n",
       "      <th>general_start_date</th>\n",
       "      <th>general_end_date</th>\n",
       "      <th>local_periods_total</th>\n",
       "      <th>local_periods_missing</th>\n",
       "      <th>extended_periods_total</th>\n",
       "      <th>extended_periods_with_data</th>\n",
       "      <th>extended_periods_missing</th>\n",
       "      <th>extended_null_sales_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.5</td>\n",
       "      <td>25.5</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.5</td>\n",
       "      <td>25.5</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.875158</td>\n",
       "      <td>14.445322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          store       item     local_start_date       local_end_date  \\\n",
       "count     500.0      500.0                  500                  500   \n",
       "mean        5.5       25.5  2013-01-01 00:00:00  2017-12-31 00:00:00   \n",
       "min         1.0        1.0  2013-01-01 00:00:00  2017-12-31 00:00:00   \n",
       "25%         3.0       13.0  2013-01-01 00:00:00  2017-12-31 00:00:00   \n",
       "50%         5.5       25.5  2013-01-01 00:00:00  2017-12-31 00:00:00   \n",
       "75%         8.0       38.0  2013-01-01 00:00:00  2017-12-31 00:00:00   \n",
       "max        10.0       50.0  2013-01-01 00:00:00  2017-12-31 00:00:00   \n",
       "std    2.875158  14.445322                  NaN                  NaN   \n",
       "\n",
       "       local_periods_with_data  local_null_sales_count   general_start_date  \\\n",
       "count                    500.0                   500.0                  500   \n",
       "mean                    1826.0                     0.0  2013-01-01 00:00:00   \n",
       "min                     1826.0                     0.0  2013-01-01 00:00:00   \n",
       "25%                     1826.0                     0.0  2013-01-01 00:00:00   \n",
       "50%                     1826.0                     0.0  2013-01-01 00:00:00   \n",
       "75%                     1826.0                     0.0  2013-01-01 00:00:00   \n",
       "max                     1826.0                     0.0  2013-01-01 00:00:00   \n",
       "std                        0.0                     0.0                  NaN   \n",
       "\n",
       "          general_end_date  local_periods_total  local_periods_missing  \\\n",
       "count                  500                500.0                  500.0   \n",
       "mean   2017-12-31 00:00:00               1826.0                    0.0   \n",
       "min    2017-12-31 00:00:00               1826.0                    0.0   \n",
       "25%    2017-12-31 00:00:00               1826.0                    0.0   \n",
       "50%    2017-12-31 00:00:00               1826.0                    0.0   \n",
       "75%    2017-12-31 00:00:00               1826.0                    0.0   \n",
       "max    2017-12-31 00:00:00               1826.0                    0.0   \n",
       "std                    NaN                  0.0                    0.0   \n",
       "\n",
       "       extended_periods_total  extended_periods_with_data  \\\n",
       "count                   500.0                       500.0   \n",
       "mean                   1826.0                      1826.0   \n",
       "min                    1826.0                      1826.0   \n",
       "25%                    1826.0                      1826.0   \n",
       "50%                    1826.0                      1826.0   \n",
       "75%                    1826.0                      1826.0   \n",
       "max                    1826.0                      1826.0   \n",
       "std                       0.0                         0.0   \n",
       "\n",
       "       extended_periods_missing  extended_null_sales_count  \n",
       "count                     500.0                      500.0  \n",
       "mean                        0.0                        0.0  \n",
       "min                         0.0                        0.0  \n",
       "25%                         0.0                        0.0  \n",
       "50%                         0.0                        0.0  \n",
       "75%                         0.0                        0.0  \n",
       "max                         0.0                        0.0  \n",
       "std                         0.0                        0.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analisis_series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67d6aea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 913000 entries, 0 to 912999\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count   Dtype         \n",
      "---  ------  --------------   -----         \n",
      " 0   date    913000 non-null  datetime64[ns]\n",
      " 1   store   913000 non-null  Int64         \n",
      " 2   item    913000 non-null  Int64         \n",
      " 3   sales   913000 non-null  Int64         \n",
      "dtypes: Int64(3), datetime64[ns](1)\n",
      "memory usage: 30.5 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03132aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "leer csv\n",
    "evaluar analisis de columnas\n",
    "EDA\n",
    "    outliers\n",
    "    distribucion\n",
    "    graficos\n",
    "procesar datos\n",
    "metodos de prediccion "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_forecast_time_series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
