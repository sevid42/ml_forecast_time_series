{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e912dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373c321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 2.3.3\n",
      "NumPy: 2.3.4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a5a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b128eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analizar_dataframe(df):\n",
    "    \"\"\"\n",
    "    An√°lisis eficiente de DataFrame usando operaciones vectorizadas.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame a analizar\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame con el an√°lisis de cada columna\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pre-calcular informaci√≥n com√∫n para todas las columnas\n",
    "    total_filas = len(df)\n",
    "    \n",
    "    # Lista para almacenar resultados\n",
    "    resultados = []\n",
    "    \n",
    "    for columna in df.columns:\n",
    "        col_data = df[columna]\n",
    "        \n",
    "        # Tipo de dato (m√°s eficiente con select_dtypes)\n",
    "        es_numerica = pd.api.types.is_numeric_dtype(col_data)\n",
    "        tipo = 'Num√©rica' if es_numerica else 'Alfanum√©rica'\n",
    "        \n",
    "        # M√°scara de nulos (calcular una sola vez)\n",
    "        mask_nulos = col_data.isna()\n",
    "        cantidad_nulos = mask_nulos.sum()\n",
    "        \n",
    "        # Valores distintos (incluidos null) - muy eficiente\n",
    "        valores_distintos = col_data.nunique(dropna=False)\n",
    "        \n",
    "        # Inicializar contadores\n",
    "        cantidad_blancos = 0\n",
    "        cantidad_ceros = 0\n",
    "        cantidad_negativos = 0\n",
    "        \n",
    "        if es_numerica:\n",
    "            # Operaciones vectorizadas para num√©ricos (muy r√°pido)\n",
    "            col_no_nulos = col_data.dropna()\n",
    "            if len(col_no_nulos) > 0:\n",
    "                cantidad_ceros = (col_no_nulos == 0).sum()\n",
    "                cantidad_negativos = (col_no_nulos < 0).sum()\n",
    "        else:\n",
    "            # Para alfanum√©ricos, usar operaciones de string vectorizadas\n",
    "            # M√°s eficiente que apply\n",
    "            if col_data.dtype == 'object':\n",
    "                # Filtrar solo strings y contar blancos en una operaci√≥n\n",
    "                mask_strings = col_data.apply(lambda x: isinstance(x, str))\n",
    "                if mask_strings.any():\n",
    "                    cantidad_blancos = col_data[mask_strings].str.strip().eq('').sum()\n",
    "        \n",
    "        # 5 valores de ejemplo (optimizado)\n",
    "        valores_unicos = col_data.dropna().unique()\n",
    "        valores_ejemplo = valores_unicos[:5] if len(valores_unicos) > 0 else []\n",
    "        valores_ejemplo_str = ', '.join(map(str, valores_ejemplo))\n",
    "        \n",
    "        # Agregar resultados\n",
    "        resultados.append({\n",
    "            'Columna': columna,\n",
    "            'Tipo': tipo,\n",
    "            'Total Valores': total_filas,\n",
    "            'Nulos': int(cantidad_nulos),\n",
    "            'Valores Distintos': int(valores_distintos),\n",
    "            'Valores Blancos': int(cantidad_blancos),\n",
    "            'Valores Cero': int(cantidad_ceros),\n",
    "            'Valores Negativos': int(cantidad_negativos),\n",
    "            'Ejemplos': valores_ejemplo_str\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "\n",
    "def analizar_csv(ruta_archivo, separador=',', encoding='utf-8', nrows=None):\n",
    "    \"\"\"\n",
    "    Lee y analiza un archivo CSV de forma eficiente.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ruta_archivo : str\n",
    "        Ruta del archivo CSV\n",
    "    separador : str, default=','\n",
    "        Separador del CSV\n",
    "    encoding : str, default='utf-8'\n",
    "        Codificaci√≥n del archivo\n",
    "    nrows : int, optional\n",
    "        N√∫mero de filas a leer (√∫til para archivos grandes)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (DataFrame original, DataFrame de an√°lisis)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Leyendo archivo: {ruta_archivo}\")\n",
    "    \n",
    "    # Leer CSV con optimizaciones\n",
    "    df = pd.read_csv(\n",
    "        ruta_archivo, \n",
    "        sep=separador, \n",
    "        encoding=encoding,\n",
    "        nrows=nrows,\n",
    "        low_memory=False  # M√°s r√°pido para archivos grandes\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Archivo cargado: {df.shape[0]:,} filas, {df.shape[1]} columnas\\n\")\n",
    "    \n",
    "    print(\"Analizando columnas...\")\n",
    "    df_analisis = analizar_dataframe(df)\n",
    "    print(\"‚úì An√°lisis completado\\n\")\n",
    "    \n",
    "    return df, df_analisis\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# VERSI√ìN ULTRA OPTIMIZADA (para Big Data)\n",
    "# ============================================\n",
    "\n",
    "def analizar_csv_chunked(ruta_archivo, chunksize=10000, separador=',', encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Analiza archivos CSV muy grandes por chunks (evita cargar todo en memoria).\n",
    "    Ideal para datasets de millones de filas.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ruta_archivo : str\n",
    "        Ruta del archivo CSV\n",
    "    chunksize : int, default=10000\n",
    "        N√∫mero de filas por chunk\n",
    "    separador : str, default=','\n",
    "        Separador del CSV\n",
    "    encoding : str, default='utf-8'\n",
    "        Codificaci√≥n del archivo\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame con el an√°lisis de cada columna\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Analizando archivo por chunks de {chunksize:,} filas...\")\n",
    "    \n",
    "    # Diccionarios para acumular estad√≠sticas\n",
    "    stats = {}\n",
    "    total_rows = 0\n",
    "    \n",
    "    # Leer por chunks\n",
    "    for i, chunk in enumerate(pd.read_csv(ruta_archivo, sep=separador, \n",
    "                                          encoding=encoding, chunksize=chunksize)):\n",
    "        \n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "        for columna in chunk.columns:\n",
    "            if columna not in stats:\n",
    "                # Inicializar estad√≠sticas para nueva columna\n",
    "                es_numerica = pd.api.types.is_numeric_dtype(chunk[columna])\n",
    "                stats[columna] = {\n",
    "                    'tipo': 'Num√©rica' if es_numerica else 'Alfanum√©rica',\n",
    "                    'nulos': 0,\n",
    "                    'valores_unicos': set(),\n",
    "                    'blancos': 0,\n",
    "                    'ceros': 0,\n",
    "                    'negativos': 0,\n",
    "                    'ejemplos': []\n",
    "                }\n",
    "            \n",
    "            col_data = chunk[columna]\n",
    "            col_stats = stats[columna]\n",
    "            \n",
    "            # Acumular nulos\n",
    "            col_stats['nulos'] += col_data.isna().sum()\n",
    "            \n",
    "            # Acumular valores √∫nicos (limitado para eficiencia)\n",
    "            if len(col_stats['valores_unicos']) < 10000:\n",
    "                col_stats['valores_unicos'].update(col_data.dropna().unique())\n",
    "            \n",
    "            # Ejemplos (solo del primer chunk)\n",
    "            if i == 0 and len(col_stats['ejemplos']) < 5:\n",
    "                ejemplos = col_data.dropna().unique()[:5].tolist()\n",
    "                col_stats['ejemplos'].extend(ejemplos)\n",
    "            \n",
    "            # Estad√≠sticas espec√≠ficas por tipo\n",
    "            if col_stats['tipo'] == 'Num√©rica':\n",
    "                col_no_nulos = col_data.dropna()\n",
    "                col_stats['ceros'] += (col_no_nulos == 0).sum()\n",
    "                col_stats['negativos'] += (col_no_nulos < 0).sum()\n",
    "            else:\n",
    "                if col_data.dtype == 'object':\n",
    "                    mask_strings = col_data.apply(lambda x: isinstance(x, str))\n",
    "                    if mask_strings.any():\n",
    "                        col_stats['blancos'] += col_data[mask_strings].str.strip().eq('').sum()\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Procesados {total_rows:,} filas...\")\n",
    "    \n",
    "    print(f\"‚úì An√°lisis completado: {total_rows:,} filas totales\\n\")\n",
    "    \n",
    "    # Construir DataFrame de resultados\n",
    "    resultados = []\n",
    "    for columna, col_stats in stats.items():\n",
    "        resultados.append({\n",
    "            'Columna': columna,\n",
    "            'Tipo': col_stats['tipo'],\n",
    "            'Total Valores': total_rows,\n",
    "            'Nulos': int(col_stats['nulos']),\n",
    "            'Valores Distintos': len(col_stats['valores_unicos']),\n",
    "            'Valores Blancos': int(col_stats['blancos']),\n",
    "            'Valores Cero': int(col_stats['ceros']),\n",
    "            'Valores Negativos': int(col_stats['negativos']),\n",
    "            'Ejemplos': ', '.join(map(str, col_stats['ejemplos'][:5]))\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# EJEMPLO DE USO\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ===== PARA ARCHIVOS NORMALES (< 1 GB) =====\n",
    "    ruta_csv = 'C:\\\\Users\\\\fierr\\\\OneDrive\\\\Documents\\\\UDEMY\\\\forecastWallmark\\\\forecast\\\\ml_forecast_time_series\\\\data\\\\raw\\\\train.csv'\n",
    "    df_original, df_analisis = analizar_csv(ruta_csv)\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(\"AN√ÅLISIS DE COLUMNAS\")\n",
    "    print(\"=\" * 100)\n",
    "    print(df_analisis.to_string(index=False))\n",
    "    \n",
    "    # ===== PARA ARCHIVOS GRANDES (> 1 GB) =====\n",
    "    # df_analisis = analizar_csv_chunked(ruta_csv, chunksize=50000)\n",
    "    # print(df_analisis.to_string(index=False))\n",
    "    \n",
    "    # Guardar resultado\n",
    "    df_analisis.to_csv('reports/analisis_columnas.csv', index=False)\n",
    "    print(\"\\n‚úì An√°lisis guardado en: reports/analisis_columnas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6225f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# ============================================\n",
    "# AN√ÅLISIS SIMPLE DE SERIES DE TIEMPO\n",
    "# ============================================\n",
    "\n",
    "def analizar_series(df, col_fecha='date', col_store='store', col_item='item'):\n",
    "    \"\"\"\n",
    "    An√°lisis r√°pido de series de tiempo por store e item.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"AN√ÅLISIS DE SERIES DE TIEMPO\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Convertir fecha si es necesario\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[col_fecha]):\n",
    "        df[col_fecha] = pd.to_datetime(df[col_fecha])\n",
    "    \n",
    "    print(f\"üìä Total registros: {len(df):,}\")\n",
    "    print(f\"üè™ Stores √∫nicos: {df[col_store].nunique():,}\")\n",
    "    print(f\"üì¶ Items √∫nicos: {df[col_item].nunique():,}\")\n",
    "    \n",
    "    # An√°lisis por (store, item)\n",
    "    analisis = df.groupby([col_store, col_item]).agg(\n",
    "        registros=pd.NamedAgg(column=col_fecha, aggfunc='count'),\n",
    "        fecha_min=pd.NamedAgg(column=col_fecha, aggfunc='min'),\n",
    "        fecha_max=pd.NamedAgg(column=col_fecha, aggfunc='max')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calcular d√≠as\n",
    "    analisis['dias_totales'] = (analisis['fecha_max'] - analisis['fecha_min']).dt.days + 1\n",
    "    analisis['dias_esperados'] = analisis['dias_totales']\n",
    "    analisis['completitud_pct'] = (analisis['registros'] / analisis['dias_esperados'] * 100).round(2)\n",
    "    analisis['registros_faltantes'] = analisis['dias_esperados'] - analisis['registros']\n",
    "    \n",
    "    print(f\"üî¢ Combinaciones (store, item): {len(analisis):,}\\n\")\n",
    "    \n",
    "    return analisis\n",
    "\n",
    "\n",
    "def analizar_por_deciles(analisis_df):\n",
    "    \"\"\"\n",
    "    Agrupa las series por deciles seg√∫n el n√∫mero de registros.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcular deciles\n",
    "    try:\n",
    "        analisis_df['decil'] = pd.qcut(\n",
    "            analisis_df['registros'], \n",
    "            q=10, \n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        # Convertir a etiquetas D1, D2, etc.\n",
    "        analisis_df['decil'] = 'D' + (analisis_df['decil'] + 1).astype(str)\n",
    "    except Exception:\n",
    "        # Si no se pueden calcular 10 deciles, usar percentiles\n",
    "        analisis_df['decil'] = pd.cut(\n",
    "            analisis_df['registros'],\n",
    "            bins=10,\n",
    "            labels=['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10']\n",
    "        )\n",
    "    \n",
    "    # Estad√≠sticas por decil\n",
    "    deciles = analisis_df.groupby('decil', observed=True).agg({\n",
    "        'store': 'count',\n",
    "        'registros': ['min', 'max', 'mean'],\n",
    "        'completitud_pct': 'mean',\n",
    "        'registros_faltantes': ['mean', 'sum']\n",
    "    }).round(2)\n",
    "    \n",
    "    deciles.columns = ['num_series', 'registros_min', 'registros_max', \n",
    "                      'registros_prom', 'completitud_prom', \n",
    "                      'faltantes_prom', 'faltantes_total']\n",
    "    \n",
    "    return deciles, analisis_df\n",
    "\n",
    "\n",
    "def mostrar_resultados(analisis_df, deciles_df):\n",
    "    \"\"\"\n",
    "    Muestra los resultados del an√°lisis.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"RESUMEN ESTAD√çSTICO\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    resumen_data = {\n",
    "        'M√©trica': [\n",
    "            'Combinaciones (store, item)',\n",
    "            'Registros promedio',\n",
    "            'Registros m√≠nimo',\n",
    "            'Registros m√°ximo',\n",
    "            'Completitud promedio',\n",
    "            'Series 100% completas',\n",
    "            'Series con gaps'\n",
    "        ],\n",
    "        'Valor': [\n",
    "            f\"{len(analisis_df):,}\",\n",
    "            f\"{analisis_df['registros'].mean():.0f}\",\n",
    "            f\"{analisis_df['registros'].min():,}\",\n",
    "            f\"{analisis_df['registros'].max():,}\",\n",
    "            f\"{analisis_df['completitud_pct'].mean():.1f}%\",\n",
    "            f\"{(analisis_df['completitud_pct'] == 100).sum():,} ({(analisis_df['completitud_pct'] == 100).sum()/len(analisis_df)*100:.1f}%)\",\n",
    "            f\"{(analisis_df['registros_faltantes'] > 0).sum():,} ({(analisis_df['registros_faltantes'] > 0).sum()/len(analisis_df)*100:.1f}%)\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(tabulate(pd.DataFrame(resumen_data), headers='keys', tablefmt='grid', showindex=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AN√ÅLISIS POR DECILES (seg√∫n n√∫mero de registros)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    print(tabulate(deciles_df, headers='keys', tablefmt='grid'))\n",
    "    \n",
    "    # Distribuci√≥n de completitud\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DISTRIBUCI√ìN DE COMPLETITUD\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    bins_completitud = [0, 50, 70, 80, 90, 95, 99, 100]\n",
    "    analisis_df['rango_completitud'] = pd.cut(\n",
    "        analisis_df['completitud_pct'], \n",
    "        bins=bins_completitud,\n",
    "        labels=['0-50%', '50-70%', '70-80%', '80-90%', '90-95%', '95-99%', '99-100%']\n",
    "    )\n",
    "    \n",
    "    dist_completitud = analisis_df['rango_completitud'].value_counts().sort_index()\n",
    "    dist_completitud_pct = (dist_completitud / len(analisis_df) * 100).round(2)\n",
    "    \n",
    "    completitud_tabla = pd.DataFrame({\n",
    "        'Rango Completitud': dist_completitud.index,\n",
    "        'Num Series': dist_completitud.values,\n",
    "        '% del Total': dist_completitud_pct.values\n",
    "    })\n",
    "    \n",
    "    print(tabulate(completitud_tabla, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FUNCI√ìN PRINCIPAL - USO R√ÅPIDO\n",
    "# ============================================\n",
    "\n",
    "def analisis_completo(df, exportar=True):\n",
    "    \"\"\"\n",
    "    Ejecuta an√°lisis completo y exporta resultados.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame con columnas: date, store, item\n",
    "    exportar : bool\n",
    "        Si True, exporta a CSV\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (analisis_df, deciles_df)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. An√°lisis por serie\n",
    "    analisis_df = analizar_series(df)\n",
    "    \n",
    "    # 2. An√°lisis por deciles\n",
    "    deciles_df, analisis_df = analizar_por_deciles(analisis_df)\n",
    "    \n",
    "    # 3. Mostrar resultados\n",
    "    mostrar_resultados(analisis_df, deciles_df)\n",
    "    \n",
    "    # 4. Exportar\n",
    "    if exportar:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXPORTANDO RESULTADOS\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        analisis_df.to_csv('analisis_series_completo.csv', index=False)\n",
    "        print(\"‚úì analisis_series_completo.csv\")\n",
    "        \n",
    "        deciles_df.to_csv('analisis_deciles.csv')\n",
    "        print(\"‚úì analisis_deciles.csv\")\n",
    "        \n",
    "        # Exportar series con problemas\n",
    "        series_problemas = analisis_df[analisis_df['completitud_pct'] < 90]\n",
    "        if len(series_problemas) > 0:\n",
    "            series_problemas.to_csv('series_incompletas.csv', index=False)\n",
    "            print(\"‚úì series_incompletas.csv\")\n",
    "    \n",
    "    return analisis_df, deciles_df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# EJEMPLO DE USO CON TU DATASET\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Ruta de tu archivo\n",
    "    ruta_archivo = r'C:\\Users\\fierr\\OneDrive\\Documents\\UDEMY\\forecastWallmark\\forecast\\ml_forecast_time_series\\data\\raw\\train.csv'\n",
    "    \n",
    "    # Schema de tipos\n",
    "    raw_schema = {\n",
    "        'store': 'Int64',\n",
    "        'item': 'Int64',\n",
    "        'sales': 'Int64'\n",
    "    }\n",
    "    \n",
    "    # Schema de fechas\n",
    "    date_schema = {\n",
    "        'date': '%Y-%m-%d'\n",
    "    }\n",
    "    \n",
    "    # Leer CSV\n",
    "    print(\"üìÇ Leyendo CSV...\\n\")\n",
    "    \n",
    "    # Opci√≥n 1: Lectura simple\n",
    "    df = pd.read_csv(ruta_archivo, dtype=raw_schema, parse_dates=['date'])\n",
    "    \n",
    "    # O usando tu funci√≥n personalizada:\n",
    "    # from tu_funcion_mejorada import leer_csv\n",
    "    # df = leer_csv(ruta_archivo, df_schema=raw_schema, date_schema=date_schema)\n",
    "    \n",
    "    print(f\"‚úì Datos cargados: {df.shape[0]:,} registros\\n\")\n",
    "    \n",
    "    # Ejecutar an√°lisis completo\n",
    "    analisis_df, deciles_df = analisis_completo(df, exportar=True)\n",
    "    \n",
    "    # ========================================\n",
    "    # CONSULTAS ADICIONALES\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CONSULTAS ADICIONALES\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Top 10 series con m√°s registros\n",
    "    print(\"üìä TOP 10 SERIES CON M√ÅS REGISTROS:\")\n",
    "    top_10 = analisis_df.nlargest(10, 'registros')[['store', 'item', 'registros', 'completitud_pct']]\n",
    "    print(tabulate(top_10, headers='keys', tablefmt='grid', showindex=False))\n",
    "    \n",
    "    print(\"\\nüìä TOP 10 SERIES CON MENOS REGISTROS:\")\n",
    "    bottom_10 = analisis_df.nsmallest(10, 'registros')[['store', 'item', 'registros', 'completitud_pct']]\n",
    "    print(tabulate(bottom_10, headers='keys', tablefmt='grid', showindex=False))\n",
    "    \n",
    "    # Series por decil\n",
    "    print(\"\\nüìä N√öMERO DE SERIES POR DECIL:\")\n",
    "    series_por_decil = analisis_df['decil'].value_counts().sort_index()\n",
    "    print(tabulate(series_por_decil.reset_index(), headers=['Decil', 'Num Series'], \n",
    "                  tablefmt='grid', showindex=False))\n",
    "    \n",
    "    # Ejemplo: filtrar serie espec√≠fica\n",
    "    print(\"\\nüìä EJEMPLO: Serie Store=1, Item=1:\")\n",
    "    serie_ejemplo = analisis_df[(analisis_df['store'] == 1) & (analisis_df['item'] == 1)]\n",
    "    if len(serie_ejemplo) > 0:\n",
    "        print(tabulate(serie_ejemplo[['store', 'item', 'registros', 'fecha_min', \n",
    "                                     'fecha_max', 'dias_totales', 'completitud_pct']], \n",
    "                      headers='keys', tablefmt='grid', showindex=False))\n",
    "    \n",
    "    # Series por rango de completitud\n",
    "    print(\"\\nüìä SERIES POR RANGO DE COMPLETITUD:\")\n",
    "    completitud_rangos = analisis_df.groupby('rango_completitud', observed=True).size()\n",
    "    print(tabulate(completitud_rangos.reset_index(), headers=['Rango', 'Cantidad'], \n",
    "                  tablefmt='grid', showindex=False))\n",
    "    \n",
    "    # Estad√≠sticas por decil\n",
    "    print(\"\\nüìä ESTAD√çSTICAS POR DECIL:\")\n",
    "    for decil in ['D1', 'D5', 'D10']:\n",
    "        if decil in analisis_df['decil'].values:\n",
    "            series_decil = analisis_df[analisis_df['decil'] == decil]\n",
    "            print(f\"\\n{decil}:\")\n",
    "            print(f\"  - Series: {len(series_decil)}\")\n",
    "            print(f\"  - Registros promedio: {series_decil['registros'].mean():.0f}\")\n",
    "            print(f\"  - Registros min-max: {series_decil['registros'].min()}-{series_decil['registros'].max()}\")\n",
    "            print(f\"  - Completitud promedio: {series_decil['completitud_pct'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f812fd84",
   "metadata": {},
   "source": [
    "# COD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04beaec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï\n",
      "‚îÇ Columna   ‚îÇ Tipo         ‚îÇ   Total Valores ‚îÇ   Nulos ‚îÇ   Valores Distintos ‚îÇ   Valores Blancos ‚îÇ   Valores Cero ‚îÇ   Valores Negativos ‚îÇ Ejemplos                                                                                                ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ date      ‚îÇ Alfanum√©rica ‚îÇ          913000 ‚îÇ       0 ‚îÇ                1826 ‚îÇ                 0 ‚îÇ              0 ‚îÇ                   0 ‚îÇ 2013-01-01 00:00:00, 2013-01-02 00:00:00, 2013-01-03 00:00:00, 2013-01-04 00:00:00, 2013-01-05 00:00:00 ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ store     ‚îÇ Num√©rica     ‚îÇ          913000 ‚îÇ       0 ‚îÇ                  10 ‚îÇ                 0 ‚îÇ              0 ‚îÇ                   0 ‚îÇ 1, 2, 3, 4, 5                                                                                           ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ item      ‚îÇ Num√©rica     ‚îÇ          913000 ‚îÇ       0 ‚îÇ                  50 ‚îÇ                 0 ‚îÇ              0 ‚îÇ                   0 ‚îÇ 1, 2, 3, 4, 5                                                                                           ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ sales     ‚îÇ Num√©rica     ‚îÇ          913000 ‚îÇ       0 ‚îÇ                 213 ‚îÇ                 0 ‚îÇ              1 ‚îÇ                   0 ‚îÇ 13, 11, 14, 10, 12                                                                                      ‚îÇ\n",
      "‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ\n"
     ]
    }
   ],
   "source": [
    "def leer_csv(ruta, separador=',', encoding='utf-8', df_schema={}, date_schema={}):\n",
    "    df = pd.read_csv(\n",
    "        ruta,\n",
    "        sep = separador,                          # Separador\n",
    "        encoding = encoding,                 # Codificaci√≥n\n",
    "        dtype = df_schema,                 # ‚Üê AQU√ç VA EL DICCIONARIO\n",
    "        #parse_dates = date,                 # Columnas de fecha\n",
    "        #na_values=valores_nulos,          # Qu√© considerar como nulo\n",
    "        low_memory = False                  # M√°s r√°pido para archivos grandes\n",
    "    )\n",
    "\n",
    "    #convertimos lo que sea fecha\n",
    "    if date_schema:\n",
    "        for col_fecha, formato in date_schema.items():\n",
    "            df[col_fecha] = pd.to_datetime(df[col_fecha], format=formato, errors='coerce')\n",
    "            #print(f\"   ‚úì {col_fecha} ‚Üí datetime ({formato})\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def analizar_csv(df):\n",
    "    # Pre-calcular informaci√≥n com√∫n para todas las columnas\n",
    "    total_filas = len(df)\n",
    "    \n",
    "    # Lista para almacenar resultados\n",
    "    resultados = []\n",
    "    \n",
    "    for columna in df.columns:\n",
    "        col_data = df[columna]\n",
    "        \n",
    "        # Tipo de dato (m√°s eficiente con select_dtypes)\n",
    "        es_numerica = pd.api.types.is_numeric_dtype(col_data)\n",
    "        tipo = 'Num√©rica' if es_numerica else 'Alfanum√©rica'\n",
    "        \n",
    "        # M√°scara de nulos (calcular una sola vez)\n",
    "        mask_nulos = col_data.isna()\n",
    "        cantidad_nulos = mask_nulos.sum()\n",
    "        \n",
    "        # Valores distintos (incluidos null) - muy eficiente\n",
    "        valores_distintos = col_data.nunique(dropna=False)\n",
    "        \n",
    "        # Inicializar contadores\n",
    "        cantidad_blancos = 0\n",
    "        cantidad_ceros = 0\n",
    "        cantidad_negativos = 0\n",
    "        \n",
    "        if es_numerica:\n",
    "            # Operaciones vectorizadas para num√©ricos (muy r√°pido)\n",
    "            col_no_nulos = col_data.dropna()\n",
    "            if len(col_no_nulos) > 0:\n",
    "                cantidad_ceros = (col_no_nulos == 0).sum()\n",
    "                cantidad_negativos = (col_no_nulos < 0).sum()\n",
    "        else:\n",
    "            # Para alfanum√©ricos, usar operaciones de string vectorizadas\n",
    "            # M√°s eficiente que apply\n",
    "            if col_data.dtype == 'object':\n",
    "                # Filtrar solo strings y contar blancos en una operaci√≥n\n",
    "                mask_strings = col_data.apply(lambda x: isinstance(x, str))\n",
    "                if mask_strings.any():\n",
    "                    cantidad_blancos = col_data[mask_strings].str.strip().eq('').sum()\n",
    "        \n",
    "        # 5 valores de ejemplo (optimizado)\n",
    "        valores_unicos = col_data.dropna().unique()\n",
    "        valores_ejemplo = valores_unicos[:5] if len(valores_unicos) > 0 else []\n",
    "        valores_ejemplo_str = ', '.join(map(str, valores_ejemplo))\n",
    "        \n",
    "        # Agregar resultados\n",
    "        resultados.append({\n",
    "            'Columna': columna,\n",
    "            'Tipo': tipo,\n",
    "            'Total Valores': total_filas,\n",
    "            'Nulos': int(cantidad_nulos),\n",
    "            'Valores Distintos': int(valores_distintos),\n",
    "            'Valores Blancos': int(cantidad_blancos),\n",
    "            'Valores Cero': int(cantidad_ceros),\n",
    "            'Valores Negativos': int(cantidad_negativos),\n",
    "            'Ejemplos': valores_ejemplo_str\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ruta_archivo = 'C:\\\\Users\\\\fierr\\\\OneDrive\\\\Documents\\\\UDEMY\\\\forecastWallmark\\\\forecast\\\\ml_forecast_time_series\\\\data\\\\raw\\\\train.csv'\n",
    "    separador=','\n",
    "    encoding='utf-8'\n",
    "\n",
    "    raw_schema={\n",
    "        'date': 'string',\n",
    "        'store': 'Int64',        # Con may√∫scula acepta nulos\n",
    "        'item': 'Int64',\n",
    "        'sales': 'Int64'\n",
    "    }\n",
    "\n",
    "    date_schema={'date': '%Y-%m-%d'}\n",
    "\n",
    "    df = leer_csv(ruta_archivo,separador,encoding,raw_schema,date_schema)\n",
    "    df_analisis= analizar_csv(df)\n",
    "    #df_analisis.to_string(index=False)\n",
    "\n",
    "    #from tabulate import tabulate\n",
    "    #print(tabulate(df_analisis, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
    "    #display(df_analisis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03132aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "leer csv\n",
    "evaluar analisis de columnas\n",
    "EDA\n",
    "    outliers\n",
    "    distribucion\n",
    "    graficos\n",
    "procesar datos\n",
    "metodos de prediccion "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_forecast_time_series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
